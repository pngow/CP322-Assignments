{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84e255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries & functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e1d3313",
   "metadata": {},
   "source": [
    "First, we import the Otto data into a Pandas DataFrame (DF). We set the column ID as the index for the DF. This will also prevent us from accidentally using it as data to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e288ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61874</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61875</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61876</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61877</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61878</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61878 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "id                                                                              \n",
       "1           1       0       0       0       0       0       0       0       0   \n",
       "2           0       0       0       0       0       0       0       1       0   \n",
       "3           0       0       0       0       0       0       0       1       0   \n",
       "4           1       0       0       1       6       1       5       0       0   \n",
       "5           0       0       0       0       0       0       0       0       0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "61874       1       0       0       1       1       0       0       0       0   \n",
       "61875       4       0       0       0       0       0       0       0       0   \n",
       "61876       0       0       0       0       0       0       0       3       1   \n",
       "61877       1       0       0       0       0       0       0       0       0   \n",
       "61878       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       feat_10  ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  \\\n",
       "id              ...                                                         \n",
       "1            0  ...        1        0        0        0        0        0   \n",
       "2            0  ...        0        0        0        0        0        0   \n",
       "3            0  ...        0        0        0        0        0        0   \n",
       "4            1  ...        0        1        2        0        0        0   \n",
       "5            0  ...        1        0        0        0        0        1   \n",
       "...        ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "61874        0  ...        1        0        0        0        0        0   \n",
       "61875        0  ...        0        2        0        0        2        0   \n",
       "61876        0  ...        0        3        1        0        0        0   \n",
       "61877        0  ...        0        0        0        0        1        0   \n",
       "61878        0  ...        0        0        0        0        0        0   \n",
       "\n",
       "       feat_91  feat_92  feat_93   target  \n",
       "id                                         \n",
       "1            0        0        0  Class_1  \n",
       "2            0        0        0  Class_1  \n",
       "3            0        0        0  Class_1  \n",
       "4            0        0        0  Class_1  \n",
       "5            0        0        0  Class_1  \n",
       "...        ...      ...      ...      ...  \n",
       "61874        0        2        0  Class_9  \n",
       "61875        0        1        0  Class_9  \n",
       "61876        0        0        0  Class_9  \n",
       "61877        3       10        0  Class_9  \n",
       "61878        0        2        0  Class_9  \n",
       "\n",
       "[61878 rows x 94 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data into a dataframe ... set id column as index ... removes it from being processed in model\n",
    "df = pd.read_csv('otto.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8677ce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 61878\n",
      "Number of classes: 9\n"
     ]
    }
   ],
   "source": [
    "#print number of instances\n",
    "print('Number of instances: {}'.format(len(df)))\n",
    "\n",
    "#print number of classes\n",
    "print('Number of classes: {}'.format(df['target'].nunique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed805f50",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "Then, we need to preprocess our data to make it suitable to be used by the models.\n",
    "\n",
    "The following steps were taken:\n",
    "<li>Handle missing values in the whole dataset - we will use imputation by assigning the missing values the mode descriptive feature level across the column</li>\n",
    "<li>Filter out rows that contains ALL zeros</li>\n",
    "<li>Convert non-numerical class labels to numbers - to do this we assigned indices to the classes and mapped the categorical values to numbers</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e918f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing values - IMPUTATION\n",
    "# assign mode of column to missing values\n",
    "adjusted_df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# filter out rows with all zero values\n",
    "adjusted_df = adjusted_df[~(adjusted_df[adjusted_df.columns[:-1]] == 0).all(axis=1)]\n",
    "\n",
    "# get descriptive features\n",
    "descriptive = adjusted_df.drop('target', axis=1).to_numpy()\n",
    "# get target features\n",
    "target = adjusted_df['target']\n",
    "\n",
    "# remove non-numerical class labels/target labels\n",
    "# create a dictionary, mapping categorical class labels to integer index values\n",
    "class_label_mapping = dict(enumerate(iterable=target.unique(), start=1))\n",
    "# reverse keys/values in the dictionary\n",
    "class_label_mapping = {value : key for (key, value) in class_label_mapping.items()}\n",
    "# get numerical target values\n",
    "target = target.replace(class_label_mapping).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b899a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 6, ..., 9, 2, 7], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data - 80% training, 20% testing\n",
    "descriptive_train, descriptive_test, target_train, target_test = train_test_split(descriptive, target, test_size=0.2, random_state=42)\n",
    "# display testing target data ... to briefly compare with outputs of predictions from the models\n",
    "target_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6c297b7",
   "metadata": {},
   "source": [
    "## Training The Models\n",
    "Now that we have the data we need for our models, we need to create our models and train them. \n",
    "\n",
    "For kNN, we need to determine the optimal parameters that will classify our dataset the best. To do this, we will use sklearn's RandomizedSearchCV to explore the optimal value of k (limited from 1-30), optimal weighting (uniform/distance) and optimal distance metric (minkowski/euclidian/manhattan) to find the model version that has the best micro f-1 score.<br>\n",
    "\n",
    "<i>Note: we limit the values of k for feasibility purposes - if we test too many values for our model, the test for optimal parameters would take too long to run. Time is also the reason why we are using RandomizedSearchCV instead of GridSearchCV, which may give us better model performance parameters.</i>\n",
    "\n",
    "Once we have the optimal parameters, we will use them in our final k Nearest Neighbours model to train the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c934dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for k Nearest Neighbours:\n",
      "{'weights': 'distance', 'n_neighbors': 8, 'metric': 'manhattan'}\n"
     ]
    }
   ],
   "source": [
    "# k Nearest Neighbours\n",
    "# determine optimal value of k ... limit range to k=1-30 to make the test for the optimal k feasible\n",
    "parameters = {\"n_neighbors\": range(1, 30), \"weights\":['uniform','distance'], \"metric\": ['minkowski','euclidean','manhattan']}\n",
    "# iterate through all parameter combinations - scoring each with the f-1 score\n",
    "gridsearch = RandomizedSearchCV(KNeighborsClassifier(), parameters, scoring='f1_micro', n_iter=10, cv=5)\n",
    "gridsearch.fit(descriptive_train, target_train)\n",
    "# obtain the parameters that give us the best f-1 score\n",
    "gridsearch.best_params_\n",
    "best_weights = gridsearch.best_params_[\"weights\"]\n",
    "best_k = gridsearch.best_params_[\"n_neighbors\"]\n",
    "best_metric = gridsearch.best_params_[\"metric\"]\n",
    "\n",
    "print(\"Optimal parameters for k Nearest Neighbours:\")\n",
    "print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6780de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 6, ..., 9, 2, 7], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize knn classifier using the optimal number of neighbours and weights ... from running several times, seems like this is the optimal k value\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k, weights=best_weights, metric=best_metric)\n",
    "# train the model\n",
    "knn.fit(descriptive_train, target_train)\n",
    "# force the model to make predictions on the testing data\n",
    "knn_predicted = knn.predict(descriptive_test)\n",
    "\n",
    "knn_predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b51481e4",
   "metadata": {},
   "source": [
    "For random forest, we need to determine the optimal number parameters that will classify our dataset the best. As what was done for kNN, we will use sklearn's RandomizedSearchCV to explore the optimal number of trees (limited from 1-30) and optimal criteria (gini index/entropy) to find the model version that has the best micro f-1 score.<br>\n",
    "\n",
    "<i>Note: we limit the potential range of the number of trees for feasibility purposes - if we test too many values for our model, the test for optimal parameters would take too long to run. Time is also the reason why we are using RandomizedSearchCV instead of GridSearchCV, which may give us better model performance parameters.</i>\n",
    "\n",
    "Once we have the optimal parametrs, we will use them in our final Random Forest model to train the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a15a5eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for random forest:\n",
      "{'n_estimators': 26, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "# determine optimal value of k\n",
    "parameters = {\"criterion\": [\"gini\", \"entropy\"], \"n_estimators\": range(1, 30)}\n",
    "# iterate through all parameter combinations - scoring each with the f-1 score\n",
    "gridsearch = RandomizedSearchCV(RandomForestClassifier(), parameters, scoring='f1_micro', n_iter=10, cv=5)\n",
    "gridsearch.fit(descriptive_train, target_train)\n",
    "# obtain the parameters that give us the best f-1 score\n",
    "gridsearch.best_params_\n",
    "best_criteria = gridsearch.best_params_[\"criterion\"]\n",
    "best_n = gridsearch.best_params_[\"n_estimators\"]\n",
    "\n",
    "print(\"Optimal parameters for Random Forest:\")\n",
    "print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e1898f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 7, 6, ..., 9, 2, 7], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize random forest using the optimal critera and number of trees ... from running several times, seems like these are optimal params\n",
    "rf = RandomForestClassifier(criterion=best_criteria, n_estimators=best_n)\n",
    "# train the model\n",
    "rf.fit(descriptive_train, target_train)\n",
    "# force the model to make predictions on the testing data\n",
    "rf_predicted = rf.predict(descriptive_test)\n",
    "\n",
    "rf_predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ab3f18",
   "metadata": {},
   "source": [
    "Finally, we will find the optimal parameters for Naive Bayes. We will use sklearn's RandomizedSearchCV to explore the optimal level of smoothing to find the model version that has the best micro f-1 score.\n",
    "\n",
    "<i>Note: we limit the potential range of smoothing values for feasibility purposes - if we test too many values for our model, the test for optimal parameters would take too long to run. Time is also the reason why we are using RandomizedSearchCV instead of GridSearchCV, which may give us better model performance parameters.</i>\n",
    "\n",
    "Once we have the optimal parametrs, we will use them in our final Naive Bayes model to train the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc595ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for Naive Bayes:\n",
      "{'var_smoothing': 0.0001873817422860383}\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "# determine optimal value of k\n",
    "parameters = {\"var_smoothing\": np.logspace(0,-9, num=100)}\n",
    "# iterate through all parameter combinations - scoring each with the f-1 score\n",
    "gridsearch = RandomizedSearchCV(GaussianNB(), parameters, scoring='f1_micro', n_iter=10, cv=5)\n",
    "gridsearch.fit(descriptive_train, target_train)\n",
    "# obtain the parameters that give us the best f-1 score\n",
    "gridsearch.best_params_\n",
    "best_smoothing = gridsearch.best_params_[\"var_smoothing\"]\n",
    "\n",
    "print(\"Optimal parameters for Naive Bayes:\")\n",
    "print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8324713d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 6, ..., 9, 2, 7], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "# initialize naive bayes (gaussian) using the optimal smoothing value\n",
    "nb = GaussianNB(priors=None, var_smoothing=best_smoothing)\n",
    "# train the model\n",
    "nb.fit(descriptive_train, target_train)\n",
    "# force the model to make predictions on the testing data\n",
    "nb_predicted = nb.predict(descriptive_test)\n",
    "\n",
    "nb_predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca0c9941",
   "metadata": {},
   "source": [
    "## Model Results\n",
    "\n",
    "Now, we can compare the models we just trained by using their micro (global) f-1 score. These results should be the optimal f-1 score as we chose the parameters that optimized the metric earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "043dd2c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f-1 scores for the 3 models are:\n",
      "k Nearest Neighbour f-score: 0.7955\n",
      "Random Forest f-score: 0.8010\n",
      "Naive Bayes f-score: 0.6242\n"
     ]
    }
   ],
   "source": [
    "# calculate f1 score ... use micro/global average (harmonic mean)\n",
    "knn_f1 = f1_score(target_test, knn_predicted, average='micro') \n",
    "rf_f1 = f1_score(target_test, rf_predicted, average='micro')\n",
    "nb_f1 = f1_score(target_test, nb_predicted, average='micro')\n",
    "\n",
    "print(\"The f-1 scores for the 3 models are:\")\n",
    "print('k Nearest Neighbour f-score: {:0.4f}'.format(knn_f1))\n",
    "print('Random Forest f-score: {:0.4f}'.format(rf_f1))\n",
    "print('Naive Bayes f-score: {:0.4f}'.format(nb_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed002bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that a model is considered to have performed well under the f-1 score metric when the value is closer to 1.\n",
      "\n",
      "Comparing the results above, we can see that the Random Forest model is the model that best classifies our dataset when each of the models used its optimal parameters.\n",
      "\n",
      "This was based on comparing the f-1 score of each model, meaning Random Forest had the best average precision (of all predictions of class x, how many of them are actually x) and recall (how many actual class x did our model capture as x) metrics of all the models.\n",
      "\n",
      "PARAMETERS\n",
      "criterion: gini, n_estimators: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"Note that a model is considered to have performed well under the f-1 score metric when the value is closer to 1.\\n\")\n",
    "\n",
    "if (knn_f1 > rf_f1 and knn_f1 > nb_f1):\n",
    "    print(\"Comparing the results above, we can see that the k Nearest Neighbours model \\\n",
    "is the model that best classifies our dataset when each of the models used its optimal parameters.\\n\\n\\\n",
    "This was based on comparing the f-1 score of each model, meaning k Nearest Neighbour had the best average precision \\\n",
    "(of all predictions of class x, how many of them are actually x) \\\n",
    "and recall (how many actual class x did our model capture as x) metrics of all the models.\\n\\n\\\n",
    "PARAMETERS\\nweights: {}, metric: {}, n_neighbours: {}\".format(best_weights, best_metric, best_k))\n",
    "elif (rf_f1 > knn_f1 and rf_f1 > nb_f1):\n",
    "    print(\"Comparing the results above, we can see that the Random Forest model \\\n",
    "is the model that best classifies our dataset when each of the models used its optimal parameters.\\n\\n\\\n",
    "This was based on comparing the f-1 score of each model, meaning Random Forest had the best average precision \\\n",
    "(of all predictions of class x, how many of them are actually x) \\\n",
    "and recall (how many actual class x did our model capture as x) metrics of all the models.\\n\\n\\\n",
    "PARAMETERS\\ncriterion: {}, n_estimators: {}\".format(best_criteria, best_n))\n",
    "elif (nb_f1 > knn_f1 and nb_f1 > rf_f1):\n",
    "    print(\"Comparing the results above, we can see that the Naive Bayes model \\\n",
    "is the model that best classifies our dataset when each of the models used its optimal parameters.\\n\\n\\\n",
    "This was based on comparing the f-1 score of each model, meaning Naive Bayes had the best average precision \\\n",
    "(of all predictions of class x, how many of them are actually x) \\\n",
    "and recall (how many actual class x instances did our model capture as being x) metrics of all the models.\\n\\n\\\n",
    "PARAMETERS\\nvar_smoothing: {}\".format(best_smoothing))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8e42091",
   "metadata": {},
   "source": [
    "In addition, we provide a comparison of the three models and how similarly they classified the instances in the testing dataset below. <br><br>\n",
    "As we can see, k Nearest Neighbour (kNN) and Random Forest (RF) produced the most similar results between the models (around 88% similarity). We also know RF performed the best of all three models from our analysis above. Since kNN seemed to have performed very similarly to RF, it should be no surprise that it also had a good f-1 score as well (see above results).<br>\n",
    "\n",
    "Naive Bayes (NB) had the least correlation with the two other models, with around 67% similarity. This makes sense as it also did not perform as well as the other two models, and had a significantly lower f-1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94bdd926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare visual representation of pair-wise correlation between the algorithms\n",
    "# create a dataframe where the columns contain the results of the three models\n",
    "model_results = pd.DataFrame([knn_predicted, rf_predicted, nb_predicted]).T\n",
    "model_results.columns = ['kNN', 'RF', 'NB']\n",
    "# correlate the results across the columns (model algorithms)\n",
    "model_correlations = model_results.corr()\n",
    "# plot the results on a heatmap\n",
    "f, ax = plt.subplots()\n",
    "heatmap = sns.heatmap(model_correlations, annot=True, fmt=\"0.4f\", linewidths=.5, ax=ax)\n",
    "\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85f0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
